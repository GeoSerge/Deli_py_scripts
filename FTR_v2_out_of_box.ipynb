{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unexpected-crazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import resample\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-recording",
   "metadata": {},
   "source": [
    "- ohe with logreg - 0.712\n",
    "- ohe with logreg with age smoothing - 0.713\n",
    "- ohe with logreg with age smoothing with license category?\n",
    "- ohe with lighGBM - 0.716\n",
    "- mean encoding with logreg - 0.6915\n",
    "- mean encoding with sampling with logreg - 0.7\n",
    "- mean encoding with lightGBM - 0.716\n",
    "- label encoding with lightGBM with over-sampling with q1+q2 - 0.683\n",
    "- label encoding with lightGBM with q1+q2 - 0.709\n",
    "- label encoding with lightGBM with q1 - 0.713\n",
    "- label encoding with lightGBM with over-sampling with q1 - 0.698\n",
    "### to do:\n",
    "- try qiwi\n",
    "- more new features: registration, registration_type, birthplace, birthplace_type\n",
    "- smoothing\n",
    "- normalization\n",
    "- sampling\n",
    "- check precision/recall\n",
    "- check what category of users model predict better and worse\n",
    "### probability of road accident depends on:\n",
    "- опыт вождения\n",
    "  - как давно человек водит машину\n",
    "    - реальный стаж в годах\n",
    "  - есть ли своя машина\n",
    "    - зарплата\n",
    "    - наличие семьи\n",
    "    - где живет: за городом/в городе; в Москве/в других городах\n",
    "  - как регулярно\n",
    "  - общий пробег в км\n",
    "- стиль вождения\n",
    "  - куда и где ездит: на работу/в магазин/в бар, в аварийно-опасном районе\n",
    "  - \"уровень культуры\"\n",
    "  - образование\n",
    "    - зарплата\n",
    "    - место жительства: в Москве - районы\n",
    "    - девайс\n",
    "  - штрафы\n",
    "  - \"горячность\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-yellow",
   "metadata": {},
   "source": [
    "### Short summary\n",
    "- Был собран датасет: отобраны только те кто хоть раз садился за руль\n",
    "- Была выбрана целевая переменная: факт попадания в ДТП\n",
    "- По пользователям из датасета были собраны данные для построения модели, следующие параметры:\n",
    "   - Пол\n",
    "   - Возраст\n",
    "   - Стаж\n",
    "   - Мобильный оператор\n",
    "   - КБМ\n",
    "   - Гражданство: на данный момент в базе данных такой информации нет, был создан скрипт для классификации гражданства РФ/не РФ\n",
    "   - Место рождения: записано в текстовом формате. Часть данных уже была распознана. Другая часть в процессе прогона через API ДаДаты\n",
    "   - Категории ВУ: наличие других категорий кроме \"В\"\n",
    "   - Дополнительный параметр: скоринг от Qiwi\n",
    "- Данные были приведены в порядок, очищены, разбиты на категории\n",
    "- Модель была обучена при помощи Логистической Регрессии и Бустинга\n",
    "- По результатам обучения тестовая выборка на валидации показала ROC-AUC в диапазоне от 0.69 до 0.71\n",
    "- При этом на валидации было замечено очень низкое значение показателя Precision: 0.0. Это означает что модель просто предсказывает что ни один человек не попадет в ДТП.\n",
    "- Был сделан вывод, что такое поведение модели обусловлено несбалансированностью выборки (в оригинальном датасете ДТПшники составляют всего 1.5% выборки, в то время как те, кто не попадал в ДТП составляют 98.5%).\n",
    "- Для устранения несбалансированности было проведено овер-сэмплирование  (Over-sampling), когда соотношение классов в выборке уравнивается или приближается к более равному\n",
    "- Также, для устранения bias был использован метод кросс-валидации.\n",
    "- На данный момент была проведена только одна итерация по обучению искусственно сбалансированной модели и были сравнены различные методы обучения модели: Логистическая Регрессия, Бустинги, Метод Случайного Леса итд.\n",
    "- Лучшие результаты показали бустинги: lightGBM и XGBoost\n",
    "- На валидации они пока показывают результаты ROC-AUC ниже 0.7 (значение ROC-AUC скорее всего пострадает при калибровке Precision), однако показатель Precision поднялся до 0.09. Это означает, что из всех пользователей по которым модель предсказывает попадание в ДТП, только 9.0% действительно попадают в ДТП, т.е. происходит т.н. переобучение (over-fitting).\n",
    "xgb ROC-AUC: 0.9493188716822317\n",
    "xgb               precision    recall  f1-score       support\n",
    "0.0            0.935797  0.831361  0.880493  37186.000000\n",
    "1.0            0.849182  0.943334  0.893786  37430.000000\n",
    "accuracy       0.887531  0.887531  0.887531      0.887531\n",
    "macro avg      0.892490  0.887348  0.887139  74616.000000\n",
    "weighted avg   0.892348  0.887531  0.887161  74616.000000\n",
    "\n",
    "xgb ROC-AUC: 0.6769898526577152\n",
    "xgb               precision    recall  f1-score       support\n",
    "0.0            0.998673  0.840939  0.913043  22375.000000\n",
    "1.0            0.090468  0.934037  0.164958    379.000000\n",
    "accuracy       0.842489  0.842489  0.842489      0.842489\n",
    "macro avg      0.544570  0.887488  0.539001  22754.000000\n",
    "weighted avg   0.983546  0.842489  0.900583  22754.000000\n",
    "- Чтобы уменьшить переобучение модели и несбалансированность в дальнейшем планируется протестировать обучение модели с уклоном на F1B-score с целью нахождения баланса между значениями Precision для 0 и 1.\n",
    "\n",
    "### TO DO:\n",
    "- Проверить, действительно ли модель хорошо предсказывает\n",
    "- Мультиколлинеарность!\n",
    "- Гибридный сэмплинг\n",
    "- Дополнить фичи (место рождения)\n",
    "- Проверить модель на всех юзерах без скора QIWI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-mounting",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "humanitarian-seating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1653596 entries, 0 to 1653595\n",
      "Data columns (total 15 columns):\n",
      " #   Column                  Non-Null Count    Dtype  \n",
      "---  ------                  --------------    -----  \n",
      " 0   user_id                 1653596 non-null  int64  \n",
      " 1   user_ext                1653596 non-null  int64  \n",
      " 2   age                     1653460 non-null  float64\n",
      " 3   exp                     1652029 non-null  float64\n",
      " 4   sex                     1653596 non-null  object \n",
      " 5   phone_code              1653506 non-null  float64\n",
      " 6   license_category        1277528 non-null  object \n",
      " 7   birth_place             1320415 non-null  object \n",
      " 8   country                 1026162 non-null  object \n",
      " 9   region                  1026164 non-null  object \n",
      " 10  city                    1026164 non-null  object \n",
      " 11  passport_citizenship    1651817 non-null  object \n",
      " 12  PassportNumber          1651817 non-null  object \n",
      " 13  PassportDepartmentCode  632036 non-null   object \n",
      " 14  PassportRegistration    650104 non-null   object \n",
      "dtypes: float64(3), int64(2), object(10)\n",
      "memory usage: 189.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/sgulbin/Work/Analysis/FTR_score_v2/v2/dataset.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-petersburg",
   "metadata": {},
   "source": [
    "### Loading additional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "velvet-clear",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KBM\n",
    "kbm = pd.read_csv('C:/Users/sgulbin/Work/Analysis/FTR_score_v2/v2/data_lib/kbm_19_05_2021.csv', header = None)\n",
    "# Naming columns, removing non-necessary columns\n",
    "kbm.columns = ['user_ext', 'kbm', 'dt']\n",
    "kbm = kbm[['user_ext', 'kbm']]\n",
    "# Mobile operators codes\n",
    "mobile_codes = pd.read_csv('C:/Users/sgulbin/Work/Analysis/FTR_score_v2/v2/data_lib/mobile_codes_lib.csv')\n",
    "# Driving license categories\n",
    "license_cat = pd.read_csv('C:/Users/sgulbin/Work/Analysis/FTR_score_v2/v2/data_lib/license_cat_lib_v2.csv')\n",
    "# Dropping duplicates\n",
    "license_cat = license_cat.drop_duplicates(subset = 'license_category')\n",
    "# Birthplaces classification. Classified using personal experience\n",
    "bp_classified = pd.read_csv('C:/Users/sgulbin/Work/Analysis/FTR_score_v2/v2/data_lib/birthplaces_classified_lib.csv')\n",
    "bp_region_classified = bp_classified[['bp_region', 'bp_region_group_detailed']]\n",
    "bp_region_classified = bp_region_classified.drop_duplicates(subset = 'bp_region')\n",
    "# Target\n",
    "tg = pd.read_csv('C:/Users/sgulbin/Work/Analysis/FTR_score_v2/v2/target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "organic-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qiwi\n",
    "q1 = pd.read_csv('C:/Users/sgulbin/Work/Analysis/Платежеспособность/QIWI_test/deli_results_23112020.csv')\n",
    "q2 = pd.read_csv('C:/Users/sgulbin/Work/Analysis/Платежеспособность/QIWI_test/v2/qiwi_exp_v2_1_results_02032021.csv')\n",
    "q3 = pd.read_csv('C:/Users/sgulbin/Work/Analysis/Платежеспособность/QIWI_test/v3/data_delimobil_results_pd_fraud.csv')\n",
    "q = q1.append(q2, ignore_index = True)\n",
    "q = q.drop_duplicates(subset = ['ID'])\n",
    "q = q[['ID', 'pd_basis_60_v3_4', 'pd_ins_kasko_v2', 'pd_ins_osago_v2_1', 'pd_ins_osago_v2_2', 'pd_basis_gm_v1']]\n",
    "q1 = q1[['ID', 'pd_basis_60_v3_4', 'pd_ins_kasko_v2', 'pd_ins_osago_v2_1', 'pd_ins_osago_v2_2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-giant",
   "metadata": {},
   "source": [
    "### Enriching the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "written-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, kbm, on = 'user_ext', how = 'left')\n",
    "df = pd.merge(df, mobile_codes, left_on = 'phone_code', right_on = 'mobile_code', how = 'left')\n",
    "df = pd.merge(df, license_cat, on = 'license_category', how = 'left')\n",
    "df = pd.merge(df, bp_region_classified, left_on = 'region', right_on = 'bp_region', how = 'left')\n",
    "df = pd.merge(df, tg, on = 'user_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "trying-cooling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qiwi\n",
    "# df = pd.merge(df, q, left_on = 'user_id', right_on = 'ID', how = 'left')\n",
    "df = pd.merge(df, q1, left_on = 'user_id', right_on = 'ID', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "literary-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['user_ext', 'phone_code', 'license_category', 'PassportNumber', 'PassportDepartmentCode',\\\n",
    "                        'mobile_code', 'license_category_en', 'country', 'birth_place', 'city', 'PassportRegistration',\\\n",
    "                        'region', 'bp_region', 'B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "flush-journal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qiwi\n",
    "df = df.drop(columns = ['ID'])\n",
    "# df = df.drop(columns = ['pd_basis_gm_v1'])\n",
    "# df = df.drop(columns = ['pd_ins_kasko_v2', 'pd_ins_osago_v2_1', 'pd_ins_osago_v2_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "completed-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qiwi\n",
    "df = df.loc[df.pd_basis_60_v3_4.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "governing-death",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 151823 entries, 84 to 1617984\n",
      "Data columns (total 18 columns):\n",
      " #   Column                    Non-Null Count   Dtype  \n",
      "---  ------                    --------------   -----  \n",
      " 0   user_id                   151823 non-null  int64  \n",
      " 1   age                       151821 non-null  float64\n",
      " 2   exp                       151815 non-null  float64\n",
      " 3   sex                       151823 non-null  object \n",
      " 4   passport_citizenship      151734 non-null  object \n",
      " 5   kbm                       129581 non-null  float64\n",
      " 6   mobile_operator           151819 non-null  object \n",
      " 7   A                         151823 non-null  object \n",
      " 8   C                         151823 non-null  object \n",
      " 9   D                         151823 non-null  object \n",
      " 10  E                         151823 non-null  object \n",
      " 11  Tm/Tb                     151823 non-null  object \n",
      " 12  bp_region_group_detailed  93689 non-null   object \n",
      " 13  target                    2462 non-null    float64\n",
      " 14  pd_basis_60_v3_4          151823 non-null  float64\n",
      " 15  pd_ins_kasko_v2           151823 non-null  float64\n",
      " 16  pd_ins_osago_v2_1         151823 non-null  float64\n",
      " 17  pd_ins_osago_v2_2         151823 non-null  float64\n",
      "dtypes: float64(8), int64(1), object(9)\n",
      "memory usage: 22.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-separate",
   "metadata": {},
   "source": [
    "### Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ignored-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning age and exp values\n",
    "df['age'] = np.where(((df.age < 18)|(df.age>65)),np.nan,df.age)\n",
    "df['exp'] = np.where(((df.exp < 0)|(df.exp>47)),np.nan,df.exp)\n",
    "df = df.dropna(axis = 'rows', subset = ['age','exp'])\n",
    "# Cleaning license category values\n",
    "df = df.dropna(axis = 'rows', subset = ['A'])\n",
    "# Categorizing KBM\n",
    "df['kbm_grouped'] = np.where(df['kbm']<0.7,'0.5+',\\\n",
    "                              np.where(df['kbm']<0.8,'0.7+',\\\n",
    "                                       np.where(df['kbm']<0.9,'0.8+',\\\n",
    "                                                np.where(df['kbm']<1,'0.9+',\\\n",
    "                                                         np.where(df['kbm'] == 1, '1',\\\n",
    "                                                                  np.where(df['kbm']<2.3,'1.4+',\\\n",
    "                                                                           np.where(df['kbm']>=2.3,'2.3+','?')))))))\n",
    "df = df.replace('?', np.NaN)\n",
    "df = df.drop(columns = ['kbm'])\n",
    "# Replacing True and False with 1 and 0\n",
    "df = df.replace([True, False],[1, 0])\n",
    "# Replacing NaN with 0 in the target column\n",
    "df['target'] = df['target'].fillna(0)\n",
    "# Driving license\n",
    "# for col in ['A', 'C', 'D', 'E', 'Tm/Tb']:\n",
    "#     df[col+'_cat'] = df[col].replace(1,col)\n",
    "# df = df.assign(license = df['A_cat'].astype(str)+'-'+df['C_cat'].astype(str)+'-'+df['D_cat'].astype(str)+'-'+\\\n",
    "#                df['E_cat'].astype(str)+'-'+df['Tm/Tb_cat'].astype(str))\n",
    "# df = df.drop(columns = ['A_cat', 'C_cat', 'D_cat', 'E_cat', 'Tm/Tb_cat'])\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-binding",
   "metadata": {},
   "source": [
    "### Mean encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "decimal-boutique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEAVING ONLY COLUMNS THAT'LL BE USED FOR THE MODEL\n",
    "features = ['mobile_operator', 'sex', 'age', 'exp', 'bp_region_group_detailed', 'passport_citizenship', 'kbm_grouped', 'A',\\\n",
    "            'B', 'C', 'D', 'E', 'Tm/Tb']\n",
    "flt = ['age', 'exp']\n",
    "encoded = ['A','C', 'D', 'E', 'Tm/Tb']\n",
    "to_encode = ['mobile_operator', 'sex', 'bp_region_group_detailed', 'passport_citizenship', 'kbm_grouped']\n",
    "target = ['target']\n",
    "df = df[features+target]\n",
    "\n",
    "# REPLACING NA VALUES WITH 'NaN'\n",
    "df = df.replace('nan', np.nan)\n",
    "for feature in to_encode:\n",
    "    df[feature] = df[feature].fillna('NaN')\n",
    "    \n",
    "# MEAN ENCODING\n",
    "mean_enc_cols = []\n",
    "for col in to_encode:\n",
    "    means_map = df.groupby(col).target.mean()\n",
    "    df[col+'_mean_enc'] = df[col].map(means_map)\n",
    "    mean_enc_cols.append(col+'_mean_enc')\n",
    "df = df[mean_enc_cols+flt+target]\n",
    "    \n",
    "# SPLITTING DATASET INTO X AND y\n",
    "df = df.reset_index(drop=True)\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-diversity",
   "metadata": {},
   "source": [
    "### One-hot encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "freelance-villa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEAVING ONLY COLUMNS THAT'LL BE USED FOR THE MODEL\n",
    "cols = list(df.columns)\n",
    "target = ['target']\n",
    "user_id = ['user_id']\n",
    "features = [x for x in cols if x not in target+user_id]\n",
    "cat_features = [x for x in list(df.loc[:, df.dtypes == object].columns) if x not in target+user_id]\n",
    "flt_features = [x for x in list(df.loc[:, df.dtypes == float].columns) if x not in target+user_id]\n",
    "encoded = ['A','C', 'D', 'E', 'Tm/Tb']\n",
    "to_encode = cat_features\n",
    "\n",
    "df = df[features+target]\n",
    "\n",
    "# REPLACING NA VALUES WITH 'NaN'\n",
    "df = df.replace('nan', np.nan)\n",
    "for feature in to_encode:\n",
    "    df[feature] = df[feature].fillna('NaN')\n",
    "    \n",
    "# SPLITTING DATASET INTO X AND y\n",
    "df = df.reset_index(drop=True)\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# ONE-HOT ENCODING\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "X_obj = X.loc[:, X.dtypes == object]\n",
    "X_enc = X[encoded]\n",
    "X_flt = X.loc[:, X.dtypes == float]\n",
    "enc.fit(X_obj)\n",
    "X = pd.DataFrame(enc.transform(X_obj).toarray())\n",
    "X = X.join(X_enc)\n",
    "X = X.join(X_flt)\n",
    "\n",
    "# CREATING VALIDATION SAMPLE\n",
    "validation_smpl = X.join(y)\n",
    "validation_smpl = validation_smpl.sample(frac = 0.15)\n",
    "X_val = validation_smpl.iloc[:,:-1]\n",
    "y_val = validation_smpl.iloc[:,-1:]\n",
    "X = X.drop(X_val.index)\n",
    "y = y.drop(y_val.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-texas",
   "metadata": {},
   "source": [
    "### Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "motivated-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVING ONE-HOT ENCODED COLUMNS\n",
    "df = df.drop(columns = ['A', 'C', 'D', 'E', 'Tm/Tb'])\n",
    "\n",
    "# LEAVING ONLY COLUMNS THAT'LL BE USED FOR THE MODEL\n",
    "cols = list(df.columns)\n",
    "target = ['target']\n",
    "user_id = ['user_id']\n",
    "features = [x for x in cols if x not in target+user_id]\n",
    "cat_features = [x for x in list(df.loc[:, df.dtypes == object].columns) if x not in target+user_id]\n",
    "flt_features = [x for x in list(df.loc[:, df.dtypes == float].columns) if x not in target+user_id]\n",
    "encoded = ['A', 'C', 'D', 'E', 'Tm/Tb']\n",
    "to_encode = cat_features\n",
    "df = df[features+target]\n",
    "\n",
    "# REPLACING NA VALUES WITH 'NaN'\n",
    "df = df.replace('nan', np.nan)\n",
    "for feature in to_encode:\n",
    "    df[feature] = df[feature].fillna('NaN')\n",
    "\n",
    "# LABEL ENCODING CATEGORICAL FEATURES\n",
    "for col in cat_features:\n",
    "    df[col] = df[col].astype('category')\n",
    "    df[col] = df[col].cat.codes\n",
    "df.info()\n",
    "    \n",
    "# SPLITTING DATASET INTO X AND y\n",
    "df = df.reset_index(drop=True)\n",
    "validation_smpl = df.sample(frac = 0.15)\n",
    "df = df.drop(validation_smpl.index)\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_validation = validation_smpl[features]\n",
    "y_validation = validation_smpl[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-jimmy",
   "metadata": {},
   "source": [
    "### Trying age modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "protected-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['f_age'] = 1923.2*(X['age']**(-1.513))\n",
    "X = X.drop(columns = ['age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-patrol",
   "metadata": {},
   "source": [
    "### Trying resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "hidden-bridges",
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTETomek(sampling_strategy = 'auto')\n",
    "X_smt, y_smt = smt.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-royalty",
   "metadata": {},
   "source": [
    "### Trying logreg out of box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "declared-cleveland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.62      0.76     37313\n",
      "         1.0       0.03      0.68      0.05       610\n",
      "\n",
      "    accuracy                           0.62     37923\n",
      "   macro avg       0.51      0.65      0.41     37923\n",
      "weighted avg       0.98      0.62      0.75     37923\n",
      "\n",
      "0.7045012659851771\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# SPLITTING X AND y TO TRAIN AND TEST SAMPLES\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 7)\n",
    "\n",
    "# # OVER-SAMPLING\n",
    "# sm = SMOTE(random_state=7)\n",
    "# X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# # CONVERTING X AND y DATAFRAMES TO ARRAYS\n",
    "# X_train = X_res.values\n",
    "# X_test = X_test.values\n",
    "# y_train = y_res.values\n",
    "# y_test = y_test.values\n",
    "\n",
    "# # UNDER-SAMPLING\n",
    "# undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "# X_train_under, y_train_under = undersample.fit_resample(X_train, y_train)\n",
    "\n",
    "# # CONVERTING X AND y DATAFRAMES TO ARRAYS\n",
    "# X_train = X_train_under.values\n",
    "# X_test = X_test.values\n",
    "# y_train = y_train_under.values\n",
    "# y_test = y_test.values\n",
    "\n",
    "# CONVERTING X AND y DATAFRAMES TO ARRAYS\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "#LogisticRegression\n",
    "logreg = LogisticRegression(solver='liblinear', random_state=0,C=0.01\\\n",
    "                           ,class_weight = 'balanced'\\\n",
    "                           ).fit(X_train, y_train.ravel())\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "y_pred_proba = logreg.predict_proba(X_test)\n",
    "y_pred_proba1 = []\n",
    "for i in y_pred_proba:\n",
    "    y_pred_proba1.append(i[1])\n",
    "\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report = pd.DataFrame(report).transpose()\n",
    "# report.to_csv('C:/Users/sgulbin/Work/Analysis/FTR_score_v2/v2/classification_reports/ohe_logreg_over-sample_q1.csv')\n",
    "    \n",
    "print(classification_report(y_test,y_pred))\n",
    "print(roc_auc_score(y_test, y_pred_proba1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-young",
   "metadata": {},
   "source": [
    "### Trying different models with Over-Sampling and Cross-Validation out of box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "tough-establishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "models = []\n",
    "# models.append(('xgb', XGBClassifier()))\n",
    "models.append(('logreg', LogisticRegression()))\n",
    "# models.append(('knn', KNeighborsClassifier()))\n",
    "# models.append(('dt', DecisionTreeClassifier()))\n",
    "# models.append(('rf', RandomForestClassifier()))\n",
    "# models.append(('lgbm', LGBMClassifier()))\n",
    "\n",
    "seed = 7\n",
    "scoring = 'roc_auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "capital-tribe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg 0.7304065649044721\n",
      "logreg               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.61      0.76     22366\n",
      "         1.0       0.03      0.68      0.06       388\n",
      "\n",
      "    accuracy                           0.62     22754\n",
      "   macro avg       0.51      0.65      0.41     22754\n",
      "weighted avg       0.97      0.62      0.75     22754\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy='minority', random_state = seed)\n",
    "smote = SMOTE(sampling_strategy=0.3, random_state = seed)\n",
    "# fit and apply the transform\n",
    "X_over, y_over = oversample.fit_resample(X, y)\n",
    "X_sm, y_sm = smote.fit_resample(X,y)\n",
    "#\n",
    "undersample = RandomUnderSampler(sampling_strategy = 1, random_state = seed)\n",
    "#\n",
    "X_under, y_under = undersample.fit_resample(X_over, y_over)\n",
    "# SPLITTING X AND y TO TRAIN AND TEST SAMPLES\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.25, random_state = seed)\n",
    "\n",
    "# model = XGBClassifier()\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    # APPLYING K-FOLD CROSS VALIDATION\n",
    "    kfold = RepeatedStratifiedKFold(n_splits = 5, random_state = 7, n_repeats = 3)\n",
    "\n",
    "    # CALCULATING PREDICTION\n",
    "    # y_pred_val = model.predict(X_validation)\n",
    "    # y_pred_binary_val = []\n",
    "    # for i in y_pred_val:\n",
    "    #     if i > 0.5:\n",
    "    #         y_pred_binary_val.append(1)\n",
    "    #     else:\n",
    "    #         y_pred_binary_val.append(0)\n",
    "\n",
    "    # SCORING FUNCTION\n",
    "    # def score(y_true, y_pred):\n",
    "    #     print(classification_report(y_true, y_pred)) # print classification report\n",
    "    #     return accuracy_score(y_true, y_pred) # return accuracy score\n",
    "\n",
    "    # def score(X, y):\n",
    "    #     y_pred_val = model.predict(X)\n",
    "    #     y_pred_binary_val = []\n",
    "    #     for i in y_pred_val:\n",
    "    #         if i > 0.5:\n",
    "    #             y_pred_binary_val.append(1)\n",
    "    #         else:\n",
    "    #             y_pred_binary_val.append(0)\n",
    "    #     return recall_score(y, y_pred_binary_val)\n",
    "\n",
    "    # VALIDATION ON OVER-SAMPLED DATASET\n",
    "    cv_results = cross_val_score(model,\n",
    "                                 X_train,\n",
    "                                 y_train,\n",
    "                                 cv = kfold,\n",
    "                                 scoring = 'roc_auc')\n",
    "    print(name, cv_results.mean())\n",
    "    # print(classification_report(X_train, y_train))\n",
    "\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    y_pred_binary_val = []\n",
    "    for i in y_pred_val:\n",
    "        if i > 0.5:\n",
    "            y_pred_binary_val.append(1)\n",
    "        else:\n",
    "            y_pred_binary_val.append(0)\n",
    "    print(name, classification_report(y_val, y_pred_binary_val))\n",
    "\n",
    "    # # VALIDATION\n",
    "    # cv_results_val = cross_val_score(model,\n",
    "    #                                  X_validation,\n",
    "    #                                  y_validation,\n",
    "    #                                  cv = kfold_val,\n",
    "    #                                  scoring = make_scorer(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "stuffed-wallace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb               precision    recall  f1-score       support\n",
      "0.0            0.935797  0.831361  0.880493  37186.000000\n",
      "1.0            0.849182  0.943334  0.893786  37430.000000\n",
      "accuracy       0.887531  0.887531  0.887531      0.887531\n",
      "macro avg      0.892490  0.887348  0.887139  74616.000000\n",
      "weighted avg   0.892348  0.887531  0.887161  74616.000000\n",
      "xgb 0.6127918786448616\n",
      "xgb               precision    recall  f1-score       support\n",
      "0.0            0.998769  0.833773  0.908843  22379.000000\n",
      "1.0            0.086444  0.938667  0.158309    375.000000\n",
      "accuracy       0.835501  0.835501  0.835501      0.835501\n",
      "macro avg      0.542606  0.886220  0.533576  22754.000000\n",
      "weighted avg   0.983733  0.835501  0.896474  22754.000000\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# from hpsklearn import HyperoptEstimator\n",
    "# from hpsklearn import any_classifier\n",
    "# from hpsklearn impoer any_preprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "oversample = RandomOverSampler(sampling_strategy='minority', random_state = seed)\n",
    "# fit and apply the transform\n",
    "X_over, y_over = oversample.fit_resample(X, y)\n",
    "# SPLITTING X AND y TO TRAIN AND TEST SAMPLES\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.25, random_state = seed)\n",
    "# MODEL\n",
    "# model = LGBMClassifier(class_weight = 'balanced')\n",
    "# CROSS-VALIDATION\n",
    "for name, model in models:\n",
    "    kfold = RepeatedStratifiedKFold(n_splits = 5, random_state = seed, n_repeats = 3)\n",
    "#     cv_results = cross_val_score(model, X_train, y_train, cv = kfold, scoring = scoring)\n",
    "#     print(name, cv_results.mean())\n",
    "    # CLASSIFICATION REPORT\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_binary = []\n",
    "    for i in y_pred:\n",
    "        if i > 0.5:\n",
    "            y_pred_binary.append(1)\n",
    "        else:\n",
    "            y_pred_binary.append(0)\n",
    "    report = classification_report(y_test, y_pred_binary, output_dict=True)\n",
    "    report = pd.DataFrame(report).transpose()\n",
    "    print(name, report)\n",
    "    # VALIDATION\n",
    "    cv_results_val = cross_val_score(model, X_validation, y_validation, cv = kfold, scoring = scoring)\n",
    "    print(name, cv_results_val.mean())\n",
    "    # CLASSIFICATION REPORT\n",
    "    y_pred_val = model.predict(X_validation)\n",
    "    y_pred_binary_val = []\n",
    "    for i in y_pred_val:\n",
    "        if i > 0.5:\n",
    "            y_pred_binary_val.append(1)\n",
    "        else:\n",
    "            y_pred_binary_val.append(0)\n",
    "    report_val = classification_report(y_validation, y_pred_binary_val, output_dict=True)\n",
    "    report_val = pd.DataFrame(report_val).transpose()\n",
    "    print(name, report_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-finger",
   "metadata": {},
   "source": [
    "### Saving True and prediction values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "hollow-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validation = y_validation.reset_index(drop = True)\n",
    "output = y_validation.join(pd.DataFrame(y_pred_binary_val, columns = ['pred']))\n",
    "output.to_csv('C:/Users/sgulbin/Work/Analysis/FTR_score_v2/v2/results/hyperopt_result1.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-affair",
   "metadata": {},
   "source": [
    "### Tuning parameters with Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "temporal-million",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 20/20 [11:33<00:00, 34.65s/trial, best loss: 0.3866666666666667]\n",
      "Best:  {'colsample_bytree': 0.71, 'gamma': 0.02, 'learning_rate': 0.02, 'max_depth': 16, 'min_child_weight': 10.0, 'n_estimators': 26, 'subsample': 0.3}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import tpe, fmin, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import fbeta_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy='minority', random_state = seed)\n",
    "smote = SMOTE(sampling_strategy='minority', random_state = seed)\n",
    "# fit and apply the transform\n",
    "X_over, y_over = oversample.fit_resample(X, y)\n",
    "#\n",
    "X_sm, y_sm = smote.fit_resample(X,y)\n",
    "#\n",
    "undersample = RandomUnderSampler(sampling_strategy = 1, random_state = seed)\n",
    "#\n",
    "X_under, y_under = undersample.fit_resample(X_over, y_over)\n",
    "# SPLITTING X AND y TO TRAIN AND TEST SAMPLES\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.25, random_state = seed)\n",
    "\n",
    "def objective(space):\n",
    "    classifier = XGBClassifier(n_estimators = space['n_estimators'],\n",
    "                            max_depth = int(space['max_depth']),\n",
    "                            learning_rate = space['learning_rate'],\n",
    "                            gamma = space['gamma'],\n",
    "                            min_child_weight = space['min_child_weight'],\n",
    "                            subsample = space['subsample'],\n",
    "                            colsample_bytree = space['colsample_bytree']\n",
    "                            )\n",
    "    \n",
    "    classifier.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    y_pred_val = model.predict(X_validation)\n",
    "    y_pred_binary_val = []\n",
    "    for i in y_pred_val:\n",
    "        if i > 0.5:\n",
    "            y_pred_binary_val.append(1)\n",
    "        else:\n",
    "            y_pred_binary_val.append(0)\n",
    "    recall = recall_score(y_validation, y_pred_binary_val)\n",
    "#     report_val = classification_report(y_validation, y_pred_binary_val)\n",
    "#     report_val = pd.DataFrame(eval(report_val)).transpose()\n",
    "#     recall = report_val.iloc[1,1]\n",
    "\n",
    "    return{'loss':1-recall,\n",
    "           'status': STATUS_OK }\n",
    "\n",
    "space = {\n",
    "    'max_depth' : hp.choice('max_depth', range(5, 30, 1)),\n",
    "    'learning_rate' : hp.quniform('learning_rate', 0.01, 0.5, 0.01),\n",
    "    'n_estimators' : hp.choice('n_estimators', range(20, 205, 5)),\n",
    "    'gamma' : hp.quniform('gamma', 0, 0.50, 0.01),\n",
    "    'min_child_weight' : hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'subsample' : hp.quniform('subsample', 0.1, 1, 0.01),\n",
    "    'colsample_bytree' : hp.quniform('colsample_bytree', 0.1, 1.0, 0.01)\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=20,\n",
    "            trials=trials)\n",
    "\n",
    "print(\"Best: \", best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-somerset",
   "metadata": {},
   "source": [
    "### Tuning parameters with Hyperopt for logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import tpe, fmin, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import fbeta_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy='minority', random_state = seed)\n",
    "smote = SMOTE(sampling_strategy='minority', random_state = seed)\n",
    "# fit and apply the transform\n",
    "X_over, y_over = oversample.fit_resample(X, y)\n",
    "#\n",
    "X_sm, y_sm = smote.fit_resample(X,y)\n",
    "#\n",
    "undersample = RandomUnderSampler(sampling_strategy = 1, random_state = seed)\n",
    "#\n",
    "X_under, y_under = undersample.fit_resample(X_over, y_over)\n",
    "# SPLITTING X AND y TO TRAIN AND TEST SAMPLES\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.25, random_state = seed)\n",
    "\n",
    "def objective(space):\n",
    "    classifier = XGBClassifier(classifier_penalty = space['classifier_penalty'],\n",
    "                            max_depth = int(space['max_depth'])\n",
    "                            )\n",
    "    \n",
    "    classifier.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    y_pred_binary_val = []\n",
    "    for i in y_pred_val:\n",
    "        if i > 0.5:\n",
    "            y_pred_binary_val.append(1)\n",
    "        else:\n",
    "            y_pred_binary_val.append(0)\n",
    "    recall = recall_score(y_val, y_pred_binary_val)\n",
    "#     report_val = classification_report(y_validation, y_pred_binary_val)\n",
    "#     report_val = pd.DataFrame(eval(report_val)).transpose()\n",
    "#     recall = report_val.iloc[1,1]\n",
    "\n",
    "    return{'loss':1-recall,\n",
    "           'status': STATUS_OK }\n",
    "\n",
    "space = {\n",
    "    'classifier__penalty' : ['l1', 'l2'],\n",
    "    'classifier__C' : np.logspace(-4, 4, 20)\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=20,\n",
    "            trials=trials)\n",
    "\n",
    "print(\"Best: \", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "earlier-michael",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 99]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [3]*(int(11/3))\n",
    "# 11%3\n",
    "a=[1,99,0]\n",
    "b = sorted(a)\n",
    "b\n",
    "# b\n",
    "# print(a.sort())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-necessity",
   "metadata": {},
   "source": [
    "### Trying lightGBM out of box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "fitted-stage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:154: UserWarning: Converting column-vector to 1d array\n",
      "  _log_warning('Converting column-vector to 1d array')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 111918, number of negative: 111918\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1669\n",
      "[LightGBM] [Info] Number of data points in the train set: 223836, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "ROC-AUC:0.6976940968580809\n"
     ]
    }
   ],
   "source": [
    "# SPLITTING X AND y TO TRAIN AND TEST SAMPLES\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 7)\n",
    "\n",
    "# OVER-SAMPLING\n",
    "sm = SMOTE(random_state=7)\n",
    "X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# CONVERTING X AND y DATAFRAMES TO ARRAYS\n",
    "X_train = X_res.values\n",
    "X_test = X_test.values\n",
    "y_train = y_res.values\n",
    "y_test = y_test.values\n",
    "\n",
    "# CONVERTING X AND y DATAFRAMES TO ARRAYS\n",
    "# X_train = X_train.values\n",
    "# X_test = X_test.values\n",
    "# y_train = y_train.values\n",
    "# y_test = y_test.values\n",
    "\n",
    "# LIGHTGBM\n",
    "d_train = lgb.Dataset(X_train, label=y_train)\n",
    "params = {}\n",
    "params['learning_rate'] = 0.003\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'binary'\n",
    "params['metric'] = 'binary_logloss'\n",
    "params['sub_feature'] = 0.5\n",
    "params['num_leaves'] = 10\n",
    "params['min_data'] = 50\n",
    "params['max_depth'] = 10\n",
    "clf = lgb.train(params, d_train, 100)\n",
    "\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "y_pred_binary = []\n",
    "for i in y_pred:\n",
    "    if i > 0.5:\n",
    "        y_pred_binary.append(1)\n",
    "    else:\n",
    "        y_pred_binary.append(0)\n",
    "        \n",
    "report = classification_report(y_test, y_pred_binary, output_dict=True)\n",
    "report = pd.DataFrame(report).transpose()\n",
    "report.to_csv('C:/Users/sgulbin/Work/Analysis/FTR_score_v2/v2/classification_reports/le_lgbm_over-sample_q1.csv')\n",
    "\n",
    "print('ROC-AUC:'+roc_auc_score(y_test, y_pred).astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-running",
   "metadata": {},
   "source": [
    "### Checking feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "polish-incentive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             0\n",
      "x0_Ж                                  0.000834\n",
      "x0_М                                  0.119244\n",
      "x1_NaN                                0.003744\n",
      "x1_Russian                           -0.203264\n",
      "x1_foreign                            0.319598\n",
      "x2_NaN                               -0.004777\n",
      "x2_Билайн                            -0.032562\n",
      "x2_Др.                                0.053010\n",
      "x2_МТС                               -0.050627\n",
      "x2_Мегафон                           -0.018773\n",
      "x2_Теле2                              0.173807\n",
      "x3_NaN                                0.086587\n",
      "x3_Азербайджан                       -0.008440\n",
      "x3_Алтайский край                     0.437855\n",
      "x3_Амурская область                   0.049029\n",
      "x3_Армения                           -0.021281\n",
      "x3_Архангельская область              0.020674\n",
      "x3_Астраханская область               0.029396\n",
      "x3_Беларусь                           0.022105\n",
      "x3_Белгородская область              -0.071112\n",
      "x3_Брянская область                   0.214433\n",
      "x3_Владимирская область               0.096607\n",
      "x3_Волгоградская область              0.102436\n",
      "x3_Вологодская область                0.195577\n",
      "x3_Воронежская область               -0.079377\n",
      "x3_Грузия                             0.164411\n",
      "x3_Еврейская автономная область      -0.022247\n",
      "x3_Забайкальский край                 0.134555\n",
      "x3_Ивановская область                 0.180250\n",
      "x3_Иркутская область                  0.067485\n",
      "x3_Кабардино-Балкарская Республика    0.532128\n",
      "x3_Казахстан                         -0.425058\n",
      "x3_Калининградская область            0.135843\n",
      "x3_Калужская область                 -0.115406\n",
      "x3_Камчатский край                   -0.028771\n",
      "x3_Карачаево-Черкесская Республика    0.008388\n",
      "x3_Кемеровская область               -0.251113\n",
      "x3_Киргизия                           0.029817\n",
      "x3_Кировская область                  0.010413\n",
      "x3_Костромская область                0.031088\n",
      "x3_Краснодарский край                -0.483074\n",
      "x3_Красноярский край                 -0.330025\n",
      "x3_Курганская область                 0.075378\n",
      "x3_Курская область                    0.263923\n",
      "x3_Латвия                            -0.000342\n",
      "x3_Ленинградская область              0.044091\n",
      "x3_Липецкая область                   0.064450\n",
      "x3_Литва                              0.063821\n",
      "x3_Магаданская область               -0.100650\n",
      "x3_Молдавия                           0.054613\n",
      "x3_Москва                             0.200064\n",
      "x3_Московская область                 0.266597\n",
      "x3_Мурманская область                -0.341698\n",
      "x3_Ненецкий автономный округ         -0.020681\n",
      "x3_Нижегородская область              0.460254\n",
      "x3_Новгородская область               0.117589\n",
      "x3_Новосибирская область             -0.347671\n",
      "x3_Омская область                    -0.273198\n",
      "x3_Оренбургская область               0.128699\n",
      "x3_Орловская область                 -0.202056\n",
      "x3_Остальной мир                     -0.043505\n",
      "x3_Пензенская область                 0.067928\n",
      "x3_Пермский край                     -0.168060\n",
      "x3_Приморский край                    0.032995\n",
      "x3_Псковская область                 -0.181805\n",
      "x3_Республика Адыгея                 -0.180511\n",
      "x3_Республика Алтай                   0.071111\n",
      "x3_Республика Башкортостан           -0.414416\n",
      "x3_Республика Бурятия                -0.104450\n",
      "x3_Республика Дагестан                0.349880\n",
      "x3_Республика Ингушетия               0.019269\n",
      "x3_Республика Калмыкия               -0.095695\n",
      "x3_Республика Карелия                -0.171192\n",
      "x3_Республика Коми                    0.292447\n",
      "x3_Республика Марий Эл                0.252355\n",
      "x3_Республика Мордовия               -0.292108\n",
      "x3_Республика Саха (Якутия)          -0.220586\n",
      "x3_Республика Северная Осетия-Алания  0.011623\n",
      "x3_Республика Татарстан               0.314225\n",
      "x3_Республика Хакасия                -0.036646\n",
      "x3_Ростовская область                -0.092298\n",
      "x3_Рязанская область                 -0.271792\n",
      "x3_Самарская область                 -0.276598\n",
      "x3_Санкт-Петербург                   -0.008677\n",
      "x3_Саратовская область                0.050028\n",
      "x3_Сахалинская область                0.038423\n",
      "x3_Свердловская область               0.006765\n",
      "x3_Смоленская область                 0.190535\n",
      "x3_Ставропольский край               -0.066234\n",
      "x3_Таджикистан                        0.391678\n",
      "x3_Тамбовская область                 0.330333\n",
      "x3_Тверская область                  -0.147927\n",
      "x3_Томская область                   -0.301114\n",
      "x3_Тульская область                  -0.243443\n",
      "x3_Туркменистан                       0.079006\n",
      "x3_Тюменская область                  0.001506\n",
      "x3_Удмуртская Республика             -0.005568\n",
      "x3_Узбекистан                        -0.005475\n",
      "x3_Украина                            0.314464\n",
      "x3_Ульяновская область                0.002908\n",
      "x3_Хабаровский край                   0.244706\n",
      "x3_Челябинская область               -0.368713\n",
      "x3_Чеченская Республика              -0.293752\n",
      "x3_Чувашская Республика              -0.134595\n",
      "x3_Чукотский автономный округ        -0.036746\n",
      "x3_Ямало-Ненецкий автономный округ    0.143250\n",
      "x3_Ярославская область               -0.089803\n",
      "x4_0.5+                              -1.043536\n",
      "x4_0.7+                              -0.615629\n",
      "x4_0.8+                              -0.537045\n",
      "x4_0.9+                              -0.030456\n",
      "x4_1                                  0.607084\n",
      "x4_1.4+                               1.209547\n",
      "x4_2.3+                               0.326705\n",
      "x4_NaN                                0.203407\n",
      "A                                    -0.063238\n",
      "C                                     0.048014\n",
      "D                                    -0.152166\n",
      "E                                    -0.391561\n",
      "Tm/Tb                                -0.068024\n",
      "age                                  -0.003209\n",
      "exp                                  -0.056620\n",
      "pd_basis_60_v3_4                      1.209483\n",
      "pd_ins_kasko_v2                       0.389146\n",
      "pd_ins_osago_v2_1                     0.367150\n",
      "pd_ins_osago_v2_2                     0.395827\n"
     ]
    }
   ],
   "source": [
    "# LOGREG\n",
    "coefs = pd.DataFrame(model.coef_)\n",
    "coefs.columns = list(enc.get_feature_names())+encoded+flt_features\n",
    "\n",
    "coefs = coefs.T\n",
    "print(coefs.to_string())\n",
    "\n",
    "# lightGBM\n",
    "# lgb.plot_importance(clf)\n",
    "\n",
    "#XGBoost\n",
    "# model.get_booster().get_score(importance_type=\"gain\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
